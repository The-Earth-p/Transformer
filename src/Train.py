import torch
import torch.nn as nn
import torch.optim as optim
import math
import time
from matplotlib import pyplot as plt
from tqdm import tqdm
import os
import random
import numpy as np
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

from TransformerModel import make_model
from DataProcess import get_tokenizer, create_translation_datasets, create_translation_dataloader, create_masks


# ===============================
# æ¨¡åž‹é…ç½®
# ===============================
CONFIG = {
    'src_vocab_size': 10000,
    'tgt_vocab_size': 10000,
    'd_model': 512,
    'nhead': 8,
    'num_encoder_layers': 6,
    'num_decoder_layers': 6,
    'dim_feedforward': 2048,
    'dropout': 0.1
}


# ===============================
# æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§
# ===============================
def check_device_consistency(model, src_input_ids, tgt_input_ids, src_mask, tgt_mask, labels):
    print("\n=== è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥ ===")
    print(f"æ¨¡åž‹å‚æ•°è®¾å¤‡: {next(model.parameters()).device}")
    print(f"æºè¾“å…¥è®¾å¤‡: {src_input_ids.device}")
    print(f"ç›®æ ‡è¾“å…¥è®¾å¤‡: {tgt_input_ids.device}")
    print(f"æºæŽ©ç è®¾å¤‡: {src_mask.device}")
    print(f"ç›®æ ‡æŽ©ç è®¾å¤‡: {tgt_mask.device}")
    print(f"æ ‡ç­¾è®¾å¤‡: {labels.device}")
    devices = [src_input_ids.device, tgt_input_ids.device, src_mask.device, tgt_mask.device, labels.device]
    if all(d == devices[0] for d in devices):
        print("âœ… æ‰€æœ‰å¼ é‡åœ¨ç›¸åŒè®¾å¤‡ä¸Š\n")
    else:
        print("âŒ å¼ é‡è®¾å¤‡ä¸ä¸€è‡´ï¼ï¼\n")


# ===============================
# éªŒè¯é˜¶æ®µ
# ===============================
def evaluate(model, dataloader, criterion, pad_token_id, device):
    model.eval()
    total_loss = 0
    total_tokens = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            src_input_ids = torch.stack([item['src_input_ids'] for item in batch]).to(device)
            tgt_input_ids = torch.stack([item['tgt_input_ids'] for item in batch]).to(device)
            labels = torch.stack([item['labels'] for item in batch]).to(device)

            # âœ… ä¿è¯ mask ä¹Ÿåœ¨åŒä¸€è®¾å¤‡ä¸Š
            src_mask, tgt_mask = create_masks(src_input_ids, tgt_input_ids, pad_token_id)
            src_mask = src_mask.to(device)
            tgt_mask = tgt_mask.to(device)

            decoder_out = model(src_input_ids, tgt_input_ids, src_mask, tgt_mask)
            logits = model.generator(decoder_out)

            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
            non_pad_tokens = (labels != pad_token_id).sum().item()
            total_loss += loss.item() * non_pad_tokens
            total_tokens += non_pad_tokens

    model.train()
    return total_loss / total_tokens if total_tokens > 0 else 0

def calculate_bleu(model, dataloader, tokenizer, device, max_len=64):
    model.eval()
    smooth = SmoothingFunction().method1
    total_bleu = 0
    count = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Calculating BLEU"):
            src_input_ids = torch.stack([item['src_input_ids'] for item in batch]).to(device)
            labels = torch.stack([item['labels'] for item in batch]).to(device)
            src_mask, _ = create_masks(src_input_ids, labels, tokenizer.pad_token_id)
            src_mask = src_mask.to(device)

            # ä½¿ç”¨è´ªå¿ƒè§£ç ï¼ˆé€æ­¥é¢„æµ‹ï¼‰
            memory = model.encode(src_input_ids, src_mask)
            ys = torch.ones(src_input_ids.size(0), 1).fill_(tokenizer.cls_token_id).type_as(src_input_ids)

            for i in range(max_len - 1):
                tgt_mask = torch.tril(torch.ones((1, ys.size(1), ys.size(1)), device=device)).bool()
                out = model.decode(memory, src_mask, ys, tgt_mask)
                prob = model.generator(out[:, -1])
                next_word = torch.argmax(prob, dim=-1).unsqueeze(1)
                ys = torch.cat([ys, next_word], dim=1)
                if (next_word == tokenizer.sep_token_id).all():
                    break

            # è®¡ç®— BLEU
            for pred, ref in zip(ys, labels):
                pred_tokens = tokenizer.convert_ids_to_tokens(pred.tolist(), skip_special_tokens=True)
                ref_tokens = tokenizer.convert_ids_to_tokens(ref.tolist(), skip_special_tokens=True)
                if len(ref_tokens) > 0 and len(pred_tokens) > 0:
                    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)
                    total_bleu += bleu
                    count += 1

    return total_bleu / count if count > 0 else 0



# ===============================
# è®­ç»ƒé˜¶æ®µ
# ===============================
def train(model, train_dataloader, val_dataloader, optimizer, criterion, pad_token_id, device, epochs=10):
    best_val_loss = float('inf')
    train_losses, val_losses = [], []
    model.train()

    for epoch in range(epochs):
        start_time = time.time()
        total_loss = 0
        total_tokens = 0

        for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{epochs}")):
            src_input_ids = torch.stack([item['src_input_ids'] for item in batch]).to(device)
            tgt_input_ids = torch.stack([item['tgt_input_ids'] for item in batch]).to(device)
            labels = torch.stack([item['labels'] for item in batch]).to(device)

            src_mask, tgt_mask = create_masks(src_input_ids, tgt_input_ids, pad_token_id)
            src_mask = src_mask.to(device)
            tgt_mask = tgt_mask.to(device)

            # é¦–æ‰¹æ‰“å°è®¾å¤‡ä¸€è‡´æ€§ä¿¡æ¯
            if epoch == 0 and batch_idx == 0:
                check_device_consistency(model, src_input_ids, tgt_input_ids, src_mask, tgt_mask, labels)

            optimizer.zero_grad()
            decoder_out = model(src_input_ids, tgt_input_ids, src_mask, tgt_mask)
            logits = model.generator(decoder_out)

            if labels.max() >= logits.size(-1):
                print(f"âš ï¸ æ ‡ç­¾è¶Šç•Œï¼labels.max={labels.max().item()} >= vocab_size={logits.size(-1)}ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡ã€‚")
                continue
            if torch.isnan(logits).any():
                print("âš ï¸ logits å‡ºçŽ° NaNï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡ã€‚")
                continue

            loss = criterion(logits.contiguous().view(-1, logits.size(-1)),
                             labels.contiguous().view(-1))
            loss.backward()
            optimizer.step()

            non_pad_tokens = (labels != pad_token_id).sum().item()
            total_loss += loss.item() * non_pad_tokens
            total_tokens += non_pad_tokens

        train_loss = total_loss / total_tokens if total_tokens > 0 else 0
        val_loss = evaluate(model, val_dataloader, criterion, pad_token_id, device)
        train_losses.append(train_loss)
        val_losses.append(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), '../results/best_transformer_model.pth')

        print(f"Epoch {epoch+1} finished in {time.time()-start_time:.2f}s")
        print(f"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

    return train_losses, val_losses


# ===============================
# ç»˜åˆ¶æŸå¤±æ›²çº¿
# ===============================
def draw_loss(data, title):
    plt.figure(figsize=(10, 8))
    x = torch.arange(1, len(data) + 1)
    plt.plot(x, data, '-')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'{title} Loss')
    path=f"../results/{title}.png"
    os.makedirs(os.path.dirname(path),exist_ok=True)
    plt.savefig(path)

def set_seed(seed=42):
    """å›ºå®šéšæœºç§å­ä»¥ä¿è¯å®žéªŒå¯å¤çŽ°"""
    import random, os, numpy as np, torch
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)
    print(f"ðŸ”’ éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")



# ===============================
# ä¸»ç¨‹åºå…¥å£
# ===============================
def main():
    set_seed(42)
    # âœ… æŒ‡å®š GPU
    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    tokenizer = get_tokenizer()
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})

#    print(f"tokenizer.vocab_size = {tokenizer.vocab_size}")
#    print(f"pad_token_id = {tokenizer.pad_token_id}")

    CONFIG['src_vocab_size'] = tokenizer.vocab_size
    CONFIG['tgt_vocab_size'] = tokenizer.vocab_size

    # æ•°æ®é›†
    train_dataset, val_dataset, test_dataset = create_translation_datasets(
        '../train.de', '../train.en', tokenizer,
        val_ratio=0.1, test_ratio=0.1, max_length=64
    )

    train_dataloader = create_translation_dataloader(train_dataset, batch_size=64, shuffle=True)
    val_dataloader = create_translation_dataloader(val_dataset, batch_size=64, shuffle=False)
    test_dataloader = create_translation_dataloader(test_dataset, batch_size=64, shuffle=False)

    # æ¨¡åž‹æž„å»º
    true_vocab_size = tokenizer.vocab_size
#    print(f"âœ… æ­£ç¡®çš„è¯æ±‡è¡¨å¤§å°: {true_vocab_size}")

    model = make_model(
        source_vocab=true_vocab_size,
        target_vocab=true_vocab_size,
        d_model=CONFIG['d_model'],
        head=CONFIG['nhead'],
        d_ff=CONFIG['dim_feedforward'],
        N=CONFIG['num_encoder_layers'],
        dropout=CONFIG['dropout']
    )

    # âœ… æ›¿æ¢åµŒå…¥å±‚å¹¶è¿ç§»åˆ° device
    model.src_embed = nn.Sequential(
        nn.Embedding(true_vocab_size, CONFIG['d_model']),
        model.src_embed[1]
    ).to(device)

    model.tgt_embed = nn.Sequential(
        nn.Embedding(true_vocab_size, CONFIG['d_model']),
        model.tgt_embed[1]
    ).to(device)

    # âœ… generator ä¹Ÿæ”¾åˆ° device
    model.generator = nn.Linear(CONFIG['d_model'], true_vocab_size).to(device)

    model = model.to(device)
#    print(f"âœ… æ¨¡åž‹è¾“å‡ºå±‚ç»´åº¦å·²ä¿®æ­£ä¸º: {model.generator.out_features}")

    optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # ===============================
    # å¼€å§‹è®­ç»ƒ
    # ===============================
    print("Starting training...")
    train_losses, val_losses = train(
        model, train_dataloader, val_dataloader,
        optimizer, criterion, tokenizer.pad_token_id, device, epochs=25
    )

    # ===============================
    # æµ‹è¯•é˜¶æ®µ
    # ===============================
    print("Evaluating on test set...")
    model.load_state_dict(torch.load('../results/best_transformer_model.pth'))
    test_loss = evaluate(model, test_dataloader, criterion, tokenizer.pad_token_id, device)
    print(f"Test Loss: {test_loss:.4f}, Perplexity: {math.exp(test_loss):.4f}")
    bleu_score = calculate_bleu(model, test_dataloader, tokenizer, device)
    print(f"ðŸŸ¢ BLEU Score: {bleu_score:.4f}")

    draw_loss(train_losses, 'Train')
    draw_loss(val_losses, 'Val')


if __name__ == "__main__":
    main()
